/**
 * Script de prueba para verificar el adaptador de proveedores LLM
 * Este script simula las llamadas que har√≠a el backend para probar ambos proveedores
 */

// Usar fetch nativo de Node.js (disponible desde v18)

// Configuraciones de prueba
const configs = {
  ollama: {
    provider: 'ollama',
    apiBase: 'http://localhost:8000/v1',
    apiKey: 'internal-llm-key-change-in-production',
    model: 'qwen2.5-instruct'
  },
  openai: {
    provider: 'openai',
    apiBase: 'https://api.openai.com/v1',
    apiKey: 'sk-your-openai-api-key-here', // Reemplazar con clave real para probar
    model: 'gpt-4o-mini'
  }
};

// Mensaje de prueba
const testMessages = [
  {
    role: 'system',
    content: 'Eres un asistente √∫til. Responde de manera concisa.'
  },
  {
    role: 'user',
    content: 'Hola, ¬øc√≥mo est√°s?'
  }
];

// Funci√≥n para probar un proveedor
async function testProvider(providerName, config) {
  console.log(`\nüß™ Probando proveedor: ${providerName.toUpperCase()}`);
  console.log(`üì° API Base: ${config.apiBase}`);
  console.log(`ü§ñ Modelo: ${config.model}`);
  
  try {
    const requestBody = {
      model: config.model,
      messages: testMessages,
      temperature: 0.1,
      max_tokens: 100,
      stream: false,
    };

    console.log(`üì§ Enviando request...`);
    
    const response = await fetch(`${config.apiBase}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.apiKey}`,
      },
      body: JSON.stringify(requestBody),
    });

    console.log(`üì• Status: ${response.status} ${response.statusText}`);

    if (!response.ok) {
      const errorText = await response.text();
      console.log(`‚ùå Error: ${errorText}`);
      return false;
    }

    const data = await response.json();
    const message = data.choices?.[0]?.message?.content;
    
    console.log(`‚úÖ Respuesta exitosa:`);
    console.log(`üí¨ Mensaje: ${message}`);
    console.log(`üìä Tokens: ${data.usage?.total_tokens || 'N/A'}`);
    
    return true;
  } catch (error) {
    console.log(`‚ùå Error de conexi√≥n: ${error.message}`);
    return false;
  }
}

// Funci√≥n principal
async function main() {
  console.log('üöÄ Iniciando pruebas de proveedores LLM...\n');
  
  const results = {};
  
  // Probar Ollama (local)
  results.ollama = await testProvider('ollama', configs.ollama);
  
  // Probar OpenAI (solo si hay API key configurada)
  if (configs.openai.apiKey !== 'sk-your-openai-api-key-here') {
    results.openai = await testProvider('openai', configs.openai);
  } else {
    console.log('\n‚ö†Ô∏è  OpenAI: API key no configurada, saltando prueba');
    results.openai = null;
  }
  
  // Resumen
  console.log('\nüìã RESUMEN DE PRUEBAS:');
  console.log('========================');
  console.log(`Ollama (local): ${results.ollama ? '‚úÖ Funcionando' : '‚ùå Error'}`);
  console.log(`OpenAI (API): ${results.openai === null ? '‚ö†Ô∏è  No probado' : results.openai ? '‚úÖ Funcionando' : '‚ùå Error'}`);
  
  if (results.ollama || results.openai) {
    console.log('\nüéâ Al menos un proveedor est√° funcionando correctamente!');
    console.log('El adaptador LlmProviderAdapter deber√≠a funcionar sin problemas.');
  } else {
    console.log('\n‚ö†Ô∏è  Ning√∫n proveedor est√° disponible actualmente.');
    console.log('Verifica que Docker est√© corriendo (para Ollama) o configura una API key de OpenAI.');
  }
  
  console.log('\nüìù Para cambiar de proveedor en el backend:');
  console.log('1. Modifica LLM_PROVIDER en el archivo .env');
  console.log('2. Configura las variables correspondientes (API_KEY, MODEL, etc.)');
  console.log('3. Reinicia el backend');
}

// Ejecutar pruebas
main().catch(console.error);
