services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: consorcios-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: consorcios_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres123
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    networks:
      - consorcios-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d consorcios_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: consorcios-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - consorcios-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # NestJS Application (Development)
  app-dev:
    build:
      context: .
      target: dev
    container_name: consorcios-app-dev
    restart: unless-stopped
    ports:
      - "3000:3000"
      - "9229:9229" # Debug port
    environment:
      NODE_ENV: development
      DATABASE_HOST: postgres
      DATABASE_PORT: 5432
      DATABASE_NAME: consorcios_db
      DATABASE_USERNAME: postgres
      DATABASE_PASSWORD: postgres123
      REDIS_HOST: redis
      REDIS_PORT: 6379
      JWT_SECRET: your-super-secret-jwt-key-change-in-production
      JWT_EXPIRES_IN: 24h
      API_VERSION: v1
      API_PREFIX: api
      # Assistant/LLM Configuration
      ASSISTANT_ENABLED: false
      ASSISTANT_DEFAULT_MODE: dry-run
      LLM_API_BASE: http://llm-gateway:8000/v1
      LLM_API_KEY: internal-llm-key-change-in-production
      LLM_MODEL: qwen2.5-instruct
      EMBED_MODEL: text-embedding-3-small
    volumes:
      - .:/app
      - /app/node_modules
    networks:
      - consorcios-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    profiles:
      - dev

  # NestJS Application (Production)
  app:
    build:
      context: .
      target: production
    container_name: consorcios-app
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      NODE_ENV: production
      DATABASE_HOST: postgres
      DATABASE_PORT: 5432
      DATABASE_NAME: consorcios_db
      DATABASE_USERNAME: postgres
      DATABASE_PASSWORD: postgres123
      REDIS_HOST: redis
      REDIS_PORT: 6379
      JWT_SECRET: your-super-secret-jwt-key-change-in-production
      JWT_EXPIRES_IN: 24h
      API_VERSION: v1
      API_PREFIX: api
      # Assistant/LLM Configuration
      ASSISTANT_ENABLED: false
      ASSISTANT_DEFAULT_MODE: dry-run
      LLM_API_BASE: http://llm-gateway:8000/v1
      LLM_API_KEY: internal-llm-key-change-in-production
      LLM_MODEL: qwen2.5-instruct
      EMBED_MODEL: text-embedding-3-small
    networks:
      - consorcios-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    profiles:
      - prod

  # pgAdmin (Optional - for database management)
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: consorcios-pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@consorcios.com
      PGADMIN_DEFAULT_PASSWORD: admin123
      PGADMIN_CONFIG_SERVER_MODE: "False"
    ports:
      - "5050:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    networks:
      - consorcios-network
    depends_on:
      - postgres
    profiles:
      - tools

  # Ollama Model Server
  ollama:
    image: ollama/ollama:latest
    container_name: consorcios-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - consorcios-network
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    profiles:
      - llm
    # Uncomment for GPU support (NVIDIA)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # LLM Gateway (OpenAI-compatible)
  llm-gateway:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: consorcios-llm-gateway
    restart: unless-stopped
    ports:
      - "8000:4000"
    environment:
      - LITELLM_MASTER_KEY=internal-llm-key-change-in-production
      - LITELLM_LOG=INFO
      - PORT=4000
    volumes:
      - ./llm-config.yaml:/app/config.yaml
    networks:
      - consorcios-network
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:4000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    profiles:
      - llm
    command: ["--config", "/app/config.yaml", "--port", "4000"]

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  pgadmin_data:
    driver: local
  ollama_data:
    driver: local

networks:
  consorcios-network:
    driver: bridge
