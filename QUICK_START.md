# ğŸš€ GuÃ­a de Inicio RÃ¡pido - Asistente IA

Esta guÃ­a te ayudarÃ¡ a levantar el sistema completo con el Asistente IA funcionando en **menos de 10 minutos**.

## ğŸ“‹ Prerrequisitos

- âœ… **Docker Desktop** instalado y corriendo
- âœ… **Node.js 18+** instalado
- âœ… **PowerShell** (Windows) o **Bash** (Linux/Mac)
- âœ… **8GB+ RAM** disponible
- âœ… **ConexiÃ³n a internet** para descargar modelos

## ğŸ¯ OpciÃ³n 1: Inicio AutomÃ¡tico (Recomendado)

### Paso 1: Ejecutar Script AutomÃ¡tico
```powershell
# En el directorio consorcios-backend
npm run assistant:start
```

Este script hace todo automÃ¡ticamente:
- âœ… Levanta PostgreSQL y Redis
- âœ… Levanta Ollama y LiteLLM
- âœ… Descarga el modelo Qwen 2.5 (7B)
- âœ… Levanta el backend con el asistente habilitado
- âœ… Verifica que todo estÃ© funcionando

### Paso 2: Verificar que Funciona
```powershell
# Verificar salud del asistente
curl http://localhost:3000/api/v1/assistant/health
```

**Â¡Listo!** El asistente ya estÃ¡ funcionando.

## ğŸ”§ OpciÃ³n 2: Paso a Paso Manual

### Paso 1: Configurar Variables de Entorno
```powershell
# Copiar configuraciÃ³n de desarrollo
copy .env.development .env
```

### Paso 2: Levantar Servicios Base
```powershell
# PostgreSQL y Redis
docker-compose --profile dev up -d postgres redis

# Esperar a que estÃ©n listos (30-60 segundos)
docker-compose logs -f postgres
```

### Paso 3: Levantar Servicios LLM
```powershell
# Ollama y LiteLLM Gateway
npm run llm:up

# Ver logs para verificar
npm run llm:logs
```

### Paso 4: Descargar Modelo LLM
```powershell
# Descargar Qwen 2.5 (7B) - toma 5-10 minutos
npm run assistant:init-llm
```

### Paso 5: Levantar Backend
```powershell
# Backend con asistente habilitado
docker-compose --profile dev up -d app-dev

# O en modo desarrollo local
npm run start:dev
```

### Paso 6: Verificar Funcionamiento
```powershell
# API Health
curl http://localhost:3000/api/v1/health

# Assistant Health
curl http://localhost:3000/api/v1/assistant/health

# LLM Gateway
curl http://localhost:8000/health
```

## ğŸ§ª Probar el Asistente

### 1. Via API (Postman/curl)
```bash
# Login para obtener JWT
curl -X POST http://localhost:3000/api/v1/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email":"admin@local.com","password":"Admin123!"}'

# Usar el asistente
curl -X POST http://localhost:3000/api/v1/assistant \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "message": "MuÃ©strame todos los edificios",
    "context": {"userId": "admin-user"}
  }'
```

### 2. Via Frontend
1. Levantar el frontend: `cd ../consorcio-fe && npm run dev`
2. Ir a: http://localhost:5173/chat
3. Hacer login con: `admin@local.com` / `Admin123!`
4. Probar el chat con el asistente

## ğŸ” Verificar Estado de Servicios

### Servicios Corriendo
```powershell
docker ps
```

DeberÃ­as ver:
- âœ… `consorcios-postgres` (Puerto 5432)
- âœ… `consorcios-redis` (Puerto 6379)
- âœ… `consorcios-ollama` (Puerto 11434)
- âœ… `consorcios-llm-gateway` (Puerto 8000)
- âœ… `consorcios-app-dev` (Puerto 3000)

### URLs de VerificaciÃ³n
- ğŸ”— **Backend API**: http://localhost:3000/api/v1/health
- ğŸ”— **Swagger UI**: http://localhost:3000/api
- ğŸ”— **Ollama**: http://localhost:11434/api/tags
- ğŸ”— **LiteLLM**: http://localhost:8000/health
- ğŸ”— **Assistant Health**: http://localhost:3000/api/v1/assistant/health

## âŒ SoluciÃ³n de Problemas

### Problema: Docker no inicia
```powershell
# Verificar Docker
docker version

# Reiniciar Docker Desktop si es necesario
```

### Problema: Puerto ocupado
```powershell
# Ver quÃ© estÃ¡ usando el puerto
netstat -ano | findstr :3000

# Matar proceso si es necesario
taskkill /PID <PID> /F
```

### Problema: Ollama no descarga modelo
```powershell
# Verificar conectividad
curl http://localhost:11434/api/tags

# Descargar manualmente
docker exec consorcios-ollama ollama pull qwen2.5:7b-instruct
```

### Problema: LiteLLM no conecta con Ollama
```powershell
# Verificar red de Docker
docker network ls

# Reiniciar servicios LLM
docker-compose restart ollama llm-gateway
```

### Problema: Backend no encuentra LLM
```powershell
# Verificar variables de entorno
docker exec consorcios-app-dev env | grep ASSISTANT

# Verificar configuraciÃ³n
curl http://localhost:8000/v1/models
```

## ğŸ›‘ Parar Todo

```powershell
# Parar todos los servicios
docker-compose --profile dev --profile llm down

# Limpiar volÃºmenes (opcional)
docker-compose down -v
```

## ğŸ“š PrÃ³ximos Pasos

1. **Explorar el Chat**: Ir a `/chat` en el frontend
2. **Probar Tool Calls**: "Crea un nuevo ticket para reparar la puerta"
3. **Ver DocumentaciÃ³n**: Leer `LLM_SETUP.md` para configuraciÃ³n avanzada
4. **Ejecutar Tests**: `npm run assistant:test:all`
5. **Configurar GPU**: Descomentar secciÃ³n GPU en `docker-compose.yml`

## ğŸ’¡ Consejos

- ğŸ”¥ **Primera vez**: Usa el script automÃ¡tico (`npm run assistant:start`)
- âš¡ **Desarrollo**: Usa `npm run dev:full` para levantar todo
- ğŸ› **Debug**: Usa `npm run llm:logs` para ver logs de LLM
- ğŸ§ª **Testing**: Usa `npm run assistant:test:regression` para probar
- ğŸ“Š **Monitoreo**: Usa Swagger UI para explorar APIs

**Â¡El asistente IA estÃ¡ listo para usar!** ğŸ‰
